import streamlit as st
import json
import os
from langchain_core.documents import Document
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

# 1. ì•ˆì „í•œ API í‚¤ ì…ë ¥
current_dir = os.path.dirname(os.path.abspath(__file__))
password_file_path = os.path.join(current_dir, "api_key_security.json")
    
with open(password_file_path, "r", encoding="utf-8") as file:
  api_key = json.load(file)

real_api_key = api_key["api_key"]

os.environ["GOOGLE_API_KEY"] = real_api_key

# 2. ì‹œìŠ¤í…œ ì´ˆê¸°í™” ë° ìºì‹± (DBì²˜ëŸ¼ ë§¤ë²ˆ ë¡œë“œí•˜ì§€ ì•Šë„ë¡ ë©”ëª¨ë¦¬ì— ìœ ì§€í•©ë‹ˆë‹¤)
@st.cache_resource
def init_rag_system():
    
    file_path = os.path.join(current_dir, "gecko_morphs.json")
    
    with open(file_path, "r", encoding="utf-8") as file:
        morph_data = json.load(file)

    docs = []
    for item in morph_data:
        page_content = f"ëª¨í”„ ì´ë¦„: {item['morph_name_kr']} ({item['morph_name_en']})\nìœ ì „ í˜•ì§ˆ: {item['genetics']}\níŠ¹ì§•: {item['description']}\nì£¼ì˜ì‚¬í•­: {item['caution']}"
        docs.append(Document(page_content=page_content, metadata={"name": item['morph_name_en']}))

    embeddings = GoogleGenerativeAIEmbeddings(model="models/gemini-embedding-001")
    vectorstore = Chroma.from_documents(documents=docs, embedding=embeddings)
    retriever = vectorstore.as_retriever(search_kwargs={"k": 2})
    
    llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash")
    template = """ë‹¹ì‹ ì€ í«í…Œì¼ ê²Œì½” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì œê³µëœ ë¬¸ë§¥(Context)ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ì •í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
    Context: {context}
    Question: {question}"""
    prompt = ChatPromptTemplate.from_template(template)

    return ({"context": retriever, "question": RunnablePassthrough()} | prompt | llm | StrOutputParser())

# --- ì›¹ í™”ë©´ UI êµ¬ì„± ì‹œì‘ ---
st.title("ğŸ¦ í«í…Œì¼ ê²Œì½” AI ë°±ê³¼ì‚¬ì „")
st.write("í«í…Œì¼ ê²Œì½”ì˜ ëª¨í”„ ì •ë³´ë‚˜ ë¸Œë¦¬ë”© ì£¼ì˜ì‚¬í•­ì„ ììœ ë¡­ê²Œ ë¬¼ì–´ë³´ì„¸ìš”!")

# ë°±ê·¸ë¼ìš´ë“œì—ì„œ RAG ì‹œìŠ¤í…œ ì¤€ë¹„
rag_chain = init_rag_system()

# ì‚¬ìš©ì ì…ë ¥ì°½ ë§Œë“¤ê¸°
user_question = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: ê³ ìŠ¤íŠ¸ ëª¨í”„ì˜ íŠ¹ì§•ì´ ë­ì•¼?)")

# ì§ˆë¬¸í•˜ê¸° ë²„íŠ¼ì„ ëˆŒë €ì„ ë•Œì˜ ë™ì‘
if st.button("AIì—ê²Œ ë¬¼ì–´ë³´ê¸°"):
    if user_question:
        with st.spinner("AIê°€ ê½ê½ì˜ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì—´ì‹¬íˆ ì°¾ê³  ìˆìŠµë‹ˆë‹¤..."):
            response = rag_chain.invoke(user_question)
            st.success("ë‹µë³€ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
            st.write(response)
    else:
        st.warning("ì§ˆë¬¸ì„ ë¨¼ì € ì…ë ¥í•´ ì£¼ì„¸ìš”!")